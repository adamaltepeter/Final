---
title: "Final Project: IBM Employee Attrition Prediction"
author: "Group 3: Adam John Altepeter, Anyu Lei, Shubkarman Singh Sidhu, Andrew Tseng, Takumi Umemaki"
date: "2021/4/12"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

***

## 01 Exploring and Preparing the Data

### Import Data

```{r}
# Import data
hr <- read.csv("IBM HR Data new.csv", na.strings = c("", "NA"))
str(hr)
summary(hr)
```

### Data Cleasing

```{r, warning=FALSE}
# Convert variables to factors or numerics
hr$Attrition <- as.factor(hr$Attrition)
hr$BusinessTravel <- as.factor(hr$BusinessTravel)
hr$DailyRate <- as.numeric(hr$DailyRate)
hr$Department <- as.factor(hr$Department)
hr$DistanceFromHome <- as.numeric(hr$DistanceFromHome)
hr$EducationField <- as.factor(hr$EducationField)
hr$Gender <- as.factor(hr$Gender)
hr$HourlyRate <- as.numeric(hr$HourlyRate)
hr$JobRole <- as.factor(hr$JobRole)
hr$JobSatisfaction <- as.numeric(hr$JobSatisfaction)
hr$MaritalStatus <- as.factor(hr$MaritalStatus)
hr$MonthlyIncome <- as.numeric(hr$MonthlyIncome)
hr$OverTime <- as.factor(hr$OverTime)
hr$PercentSalaryHike <- as.numeric(hr$PercentSalaryHike)
hr$Employee.Source <- as.factor(hr$Employee.Source)

# Remove non-relevant variables
hr$EmployeeCount <- NULL
hr$Application.ID <- NULL
hr$EmployeeNumber <- NULL
hr$Over18 <- NULL
hr$StandardHours <- NULL

# Remove all rows contain "Test"
hr <- hr[hr$EducationField != "Test", ]
hr <- hr[hr$Employee.Source != "Test", ]

# Remove all rows with NA values
hr <- hr[complete.cases(hr),]
```

```{r}
# Summary and structure of dataframe after cleaning
summary(hr)
str(hr)
```

### Get Data Ready for Analysis

```{r}
# Convert Attrition variable to Yes/No levels
hr$Attrition <- as.factor(ifelse(hr$Attrition == "Voluntary Resignation", "Yes", "No"))

# Convert all factors into dummy variables as the input into ANN has to be numeric
hr_mm <- as.data.frame(model.matrix(~.-1,hr))

# Both AttritionYes and AttritionNo are created when converting to dummies, so we remove AttritionNo column
hr_mm$AttritionNo <- NULL

# Remove the columns with NaN values
hr_mm$EducationFieldTest <- NULL
hr_mm$Gender2 <- NULL
hr_mm$JobRole5 <- NULL
hr_mm$OverTimeY <- NULL
hr_mm$Employee.Source2 <- NULL
hr_mm$Employee.SourceTest <- NULL

# Normalize the data for ANN and KNN
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

hr_norm <- as.data.frame(lapply(hr_mm, normalize))

# Summary of data for Logistic Regression, Decision Tree and Support Vector Machine Models
str(hr_mm)
summary(hr_mm)

# Summary of data for ANN and KNN
str(hr_norm)
summary(hr_norm)
```

### Get Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(hr_mm), 10000) 

# Create a train set and test set for Logistic Regression, Decision Tree and Support Vector Machine Models
hr_train <- hr_mm[-test_set, ]
hr_test <- hr_mm[test_set, ]

# Create a normalized train set and test set for ANN and KNN
hr_norm_train <- hr_norm[-test_set, ]
hr_norm_test <- hr_norm[test_set, ]
```

### Load Library

```{r}
library(caret)
library(neuralnet)
library(class)
library(C50)
library(kernlab)
```

***

## 02 Logistic Regression

### Train Model on Data

> Firstly, we run a logistic regression on all independent variables

```{r}
# Logistic regression model
lr_01 <- glm(AttritionYes ~ ., data = hr_train, family = "binomial")
summary(lr_01)
```

### Improve Model Fit using Stepwise Selection

> Then, we use stepwise regression to find the optimal subset of independent variables

```{r, eval=FALSE}
# Perform backward stepwise selection
lr_02 <- step(lr_01, direction = "backward")
```

```{r}
# Second logistic regression model with the best subset from stepwise regression
lr_02 <- glm(AttritionYes ~ Age + BusinessTravelTravel_Frequently + 
               BusinessTravelTravel_Rarely + DailyRate + `DepartmentResearch & Development` + 
               DistanceFromHome + `EducationFieldHuman Resources` + `EducationFieldLife Sciences` + 
               EducationFieldMarketing + EducationFieldMedical + EducationFieldOther + 
               EnvironmentSatisfaction + GenderFemale + HourlyRate + JobInvolvement + 
               `JobRoleHealthcare Representative` + JobRoleManager + `JobRoleManufacturing Director` + 
               `JobRoleResearch Director` + `JobRoleResearch Scientist` + 
               `JobRoleSales Executive` + JobSatisfaction + MaritalStatusDivorced + 
               MaritalStatusMarried + NumCompaniesWorked + OverTimeYes + 
               PercentSalaryHike + PerformanceRating + RelationshipSatisfaction + 
               StockOptionLevel + TrainingTimesLastYear + WorkLifeBalance + 
               YearsInCurrentRole + YearsSinceLastPromotion + Employee.SourceAdzuna + 
               `Employee.SourceCompany Website` + Employee.SourceGlassDoor + 
               Employee.SourceIndeed + Employee.SourceJora + Employee.SourceReferral, 
             data = hr_train, family = "binomial")
summary(lr_02)
```

### Predict and Evaluate Model Performance

```{r}
# Predict probability on tele_test data based on model
predict.lr <- predict(lr_02, newdata = hr_test, type = "response")

# Classify predicted probability using CutOff = 0.25
lr_predict <- ifelse(predict.lr > 0.25, 1, 0)

# Confusion Matrix
Conf_Matrix_LR <- confusionMatrix(as.factor(lr_predict), as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_LR

# Kappa Statistics
Conf_Matrix_LR$overall["Kappa"]
```

***

## 03 ANN Model

### Train Model on Data

```{r, cache=TRUE}
# ANN with one hidden neuron
ann_01 <- neuralnet(formula = AttritionYes ~ ., data = hr_norm_train, stepmax=1e7)
```

### Predict and Evaluate Model Performance

```{r}
# Obtain model results
ann_model_results <- compute(ann_01, hr_norm_test)

# Classify predicted probabilities using CutOff = 0.4
ann_predict <- ifelse(ann_model_results$net.result > 0.4, 1, 0)

# Confusion Matrix
Conf_Matrix_ANN <- confusionMatrix(as.factor(ann_predict), as.factor(hr_norm_test$AttritionYes), positive = "1")
Conf_Matrix_ANN

# Kappa Statistics
Conf_Matrix_ANN$overall["Kappa"]
```

***

## 04 KNN Model

### Train Model on Data

> Firstly, we build a KNN model by setting k equals the square root of the size of train data

```{r}
# Exclude target variable from train and test data
hr_norm_train_knn <- hr_norm_train
hr_norm_train_knn$AttritionYes <- NULL

hr_norm_test_knn <- hr_norm_test
hr_norm_test_knn$AttritionYes <- NULL

# Store class labels for train and test data
hr_norm_train_labels <- hr_norm_train$AttritionYes
hr_norm_test_labels <- hr_norm_test$AttritionYes

# Find k value using the square root of the size of train data
sqrt(13217)

# Use KNN to classify test data, k = 115
knn_predict_01 <- knn(train = hr_norm_train_knn, test = hr_norm_test_knn, 
                      cl = hr_norm_train_labels, k=115)
```

### Predict and Evaluate Model Performance

```{r}
# Confusion Matrix
Conf_Matrix_KNN_01 <- confusionMatrix(knn_predict_01, as.factor(hr_norm_test_labels), positive = "1")
Conf_Matrix_KNN_01

# Kappa Statistics
Conf_Matrix_KNN_01$overall["Kappa"]
```

### Prediction Improvement

> Then, we find another k to improve model predictions

```{r}
# Try different k values. Smaller k leads to lowered False Negative and higher Kappa, so we use k = 7
knn_predict_02 <- knn(train = hr_norm_train_knn, test = hr_norm_test_knn, 
                      cl = hr_norm_train_labels, k=7)

# Confusion Matrix
Conf_Matrix_KNN_02 <- confusionMatrix(knn_predict_02, as.factor(hr_norm_test_labels), positive = "1")
Conf_Matrix_KNN_02

# Kappa Statistics
Conf_Matrix_KNN_02$overall["Kappa"]
```

***

## 05 Decision Tree Model

### Train Model on Data

```{r}
# Convert response variable to factor
hr_train$AttritionYes <- as.factor(hr_train$AttritionYes)

# Decision tree model
tree_model <- C5.0(AttritionYes ~ ., data = hr_train)
```

```{r, eval=FALSE}
summary(tree_model)
```

### Predict and Evaluate Model Performance

```{r}
# Predict values
tree_predict <- predict(tree_model, hr_test)

# Confusion Matrix
Conf_Matrix_Tree <- confusionMatrix(tree_predict, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_Tree

# Kappa Statistics
Conf_Matrix_Tree$overall["Kappa"]
```

### Balance Between Different Errors

```{r}
# We want to lower False Negative (people predicted of staying at the company but actually leave)
# Cost of Error (Assigning a weight)
error_cost <- matrix(c(0, 1, 3, 0), nrow = 2)
error_cost

# New decision tree model by taking account cost of error
tree_cost_model <- C5.0(AttritionYes ~ ., data = hr_train, costs = error_cost)

# New predictions
tree_cost_predict <- predict(tree_cost_model, hr_test)

# Confusion Matrix of new model
Conf_Matrix_Tree_Cost <- confusionMatrix(tree_cost_predict, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_Tree_Cost

# Kappa Statistics of new model
Conf_Matrix_Tree_Cost$overall["Kappa"]
```

***

## 06 Support Vector Machine Model

### Train Model on Data

```{r}
# First SVM model (linear)
SVM_model_vani <- ksvm(AttritionYes ~ ., data = hr_train, kernel = "vanilladot")
SVM_model_vani
```

### Predict and Evaluate Model Performance

```{r}
# Predict values
SVM_predict_vani <- predict(SVM_model_vani, hr_test)

# Confusion Matrix
Conf_Matrix_SVM_vani <- confusionMatrix(SVM_predict_vani, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_SVM_vani

# Kappa Statistics
Conf_Matrix_SVM_vani$overall["Kappa"]
```

### Prediction Improvement

```{r}
# Second SVM model (radial basis)
SVM_model_rbf <- ksvm(AttritionYes ~ ., data = hr_train, kernel = "rbfdot")

# Predict values
SVM_predict_rbf <- predict(SVM_model_rbf, hr_test)

# Confusion Matrix
Conf_Matrix_SVM_rbf <- confusionMatrix(SVM_predict_rbf, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_SVM_rbf

# Kappa Statistics
Conf_Matrix_SVM_rbf$overall["Kappa"]
```

***

## 07 Stacked Model

### Create A New Dataframe

```{r}
# Combine predictions to a new dataframe
combined <- data.frame(ann=as.numeric(as.character(ann_predict)), knn=as.numeric(as.character(knn_predict_02)), log=as.numeric(as.character(lr_predict)), dt=as.numeric(as.character(tree_cost_predict)), svm=as.numeric(as.character(SVM_predict_rbf)), actual=hr_test$AttritionYes)
summary(combined)

# Selects 3000 random rows for test data
set.seed(12345)
combinedtest_set <- sample(1:nrow(combined), 3000) 

# Create a train set and test set for stacked model
combined_train <- combined[-combinedtest_set, ]
combined_test <- combined[combinedtest_set, ]
```

### Stacked Model

```{r}
# Convert response variable (combined$AttritionYes) to factor
combined_train$actual <- as.factor(combined_train$actual)

# Stacked Model (Secondary Decision Tree Model)
stacked <- C5.0(actual ~ ., data=combined_train)
summary(stacked)

# Predictions
final_predict <- predict(stacked, combined_test)

# Confusion Matrix
Conf_Matrix_Stacked <- confusionMatrix(final_predict, as.factor(combined_test$actual),positive = "1")
Conf_Matrix_Stacked

# Kappa Statistics
Conf_Matrix_Stacked$overall["Kappa"]
```

***

## Comparison of Models

### Accuracy

> **Decision Tree Model** and **Stacked Model** have higher overall accuracy in predictions

```{r}
# Logistic Regression Model
Conf_Matrix_LR$overall["Accuracy"]
# ANN Model
Conf_Matrix_ANN$overall["Accuracy"]
# KNN Model
Conf_Matrix_KNN_02$overall["Accuracy"]
# Decision Tree Model
Conf_Matrix_Tree_Cost$overall["Accuracy"]
# SVM Model
Conf_Matrix_SVM_rbf$overall["Accuracy"]
# Stacked Model
Conf_Matrix_Stacked$overall["Accuracy"]
```

### Kappa Statistics

> **Stacked Model** has the highest Kappa

```{r}
# Logistic Regression Model
Conf_Matrix_LR$overall["Kappa"]
# ANN Model
Conf_Matrix_ANN$overall["Kappa"]
# KNN Model
Conf_Matrix_KNN_02$overall["Kappa"]
# Decision Tree Model
Conf_Matrix_Tree_Cost$overall["Kappa"]
# SVM Model
Conf_Matrix_SVM_rbf$overall["Kappa"]
# Stacked Model
Conf_Matrix_Stacked$overall["Kappa"]
```

### False Negative

> **Stacked Model** has the lowest False Negative (predict stay but actually leave the company)

```{r}
# Logistic Regression Model
Conf_Matrix_LR$table[1, 2]
# ANN Model
Conf_Matrix_ANN$table[1, 2]
# KNN Model
Conf_Matrix_KNN_02$table[1, 2]
# Decision Tree Model
Conf_Matrix_Tree_Cost$table[1, 2]
# SVM Model
Conf_Matrix_SVM_rbf$table[1, 2]
# Stacked Model
Conf_Matrix_Stacked$table[1, 2]
```

***

## Conclusion







