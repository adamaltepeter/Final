---
title: "Final Project"
author: "Takumi Umemaki"
date: "2021/4/12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Final Project: IBM Employee Attrition Prediction

### Import Data
```{r}
hr <- read.csv("IBM HR Data new.csv")
str(hr)
summary(hr)

#Data Cleansing

hr = hr[hr$Attrition == "Current employee" | hr$Attrition=="Voluntary Resignation",]
hr$Attrition <- as.factor(hr$Attrition)

hr$BusinessTravel <- as.factor(hr$BusinessTravel)
hr$Department <- as.factor(hr$Department)
hr$DistanceFromHome <- as.numeric(hr$DistanceFromHome)
hr$DistanceFromHome <- ifelse(is.na(hr$DistanceFromHome),mean(hr$DistanceFromHome,na.rm = TRUE),hr$DistanceFromHome)
str(hr$JobSatisfaction)

hr$EducationField <- as.factor(hr$EducationField)

hr$EmployeeCount <- NULL
hr$EmployeeNumber <- as.numeric(hr$EmployeeNumber)
hr$EmployeeNumber <- ifelse(is.na(hr$EmployeeNumber),mean(hr$EmployeeNumber,na.rm = TRUE),hr$EmployeeNumber)

hr$Application.ID <- NULL

hr$Gender <- as.factor(hr$Gender)
hr$JobRole <- as.factor(hr$JobRole)

hr$JobSatisfaction <- as.numeric(hr$JobSatisfaction)
hr$JobSatisfaction <- ifelse(is.na(hr$JobSatisfaction),mean(hr$JobSatisfaction,na.rm = TRUE),hr$JobSatisfaction)
hr$MaritalStatus <- as.factor(hr$MaritalStatus)

hr$MonthlyIncome <- as.numeric(hr$MonthlyIncome)
hr$MonthlyIncome <- ifelse(is.na(hr$MonthlyIncome),mean(hr$MonthlyIncome,na.rm = TRUE),hr$MonthlyIncome)


hr$Age <- ifelse(is.na(hr$Age),mean(hr$Age,na.rm = TRUE),hr$Age)
hr$DailyRate <- ifelse(is.na(hr$DailyRate),mean(hr$DailyRate,na.rm = TRUE),hr$DailyRate)
hr$EnvironmentSatisfaction <- ifelse(is.na(hr$EnvironmentSatisfaction),mean(hr$EnvironmentSatisfaction,na.rm = TRUE),hr$EnvironmentSatisfaction)
hr$JobInvolvement <- ifelse(is.na(hr$JobInvolvement),mean(hr$JobInvolvement,na.rm = TRUE),hr$JobInvolvement)
hr$NumCompaniesWorked <- ifelse(is.na(hr$NumCompaniesWorked),mean(hr$NumCompaniesWorked,na.rm = TRUE),hr$NumCompaniesWorked)
hr$StockOptionLevel <- ifelse(is.na(hr$StockOptionLevel),mean(hr$StockOptionLevel,na.rm = TRUE),hr$StockOptionLevel)
hr$TrainingTimesLastYear <- ifelse(is.na(hr$TrainingTimesLastYear),mean(hr$TrainingTimesLastYear,na.rm = TRUE),hr$TrainingTimesLastYear)
hr$WorkLifeBalance <- ifelse(is.na(hr$WorkLifeBalance),mean(hr$WorkLifeBalance,na.rm = TRUE),hr$WorkLifeBalance)
hr$YearsInCurrentRole <- ifelse(is.na(hr$YearsInCurrentRole),mean(hr$YearsInCurrentRole,na.rm = TRUE),hr$YearsInCurrentRole)
hr$YearsSinceLastPromotion <- ifelse(is.na(hr$YearsSinceLastPromotion),mean(hr$YearsSinceLastPromotion,na.rm = TRUE),hr$YearsSinceLastPromotion)

hr$Over18 <- as.factor(hr$Over18)
hr$OverTime <- as.factor(hr$OverTime)
hr$PercentSalaryHike <- as.numeric(hr$PercentSalaryHike)
hr$PercentSalaryHike <- ifelse(is.na(hr$PercentSalaryHike),mean(hr$PercentSalaryHike,na.rm = TRUE),hr$PercentSalaryHike)

hr$StandardHours <- NULL

hr$Employee.Source <- as.factor(hr$Employee.Source)

summary(hr)

```

## Test and train data
```{r}

set.seed(12345)
hr_random1 <- sample(1:nrow(hr),8000)

hr_train <- hr[-hr_random1, ]
hr_test <- hr[hr_random1, ]

```

## Load libraries

```{r}
library("C50")
library("gmodels")
library(caret)
library(neuralnet)
library(class)
```


```{r}

LRM <- glm(Attrition ~ ., data = hr_train, family = "binomial")

summary(LRM)

LRM2 <- glm(Attrition ~ Age + DailyRate + DistanceFromHome + EnvironmentSatisfaction + JobInvolvement + JobSatisfaction + NumCompaniesWorked + PercentSalaryHike + StockOptionLevel + TrainingTimesLastYear + WorkLifeBalance + YearsInCurrentRole + YearsSinceLastPromotion, data = hr_train, family = "binomial", na.action = na.omit)

summary(LRM2)

#Create a predict variable
LR_predict <- ifelse(predict(LRM2, hr_test, type = "response")<0.3,"Current employee","Voluntary Resignation")
LR_predict <- as.factor(LR_predict)

summary(LR_predict)
summary(hr_test$Attrition)

#Create confusion matrix
str(hr_test$Attrition)
confusionMatrix(LR_predict,hr_test$Attrition,positive = "Voluntary Resignation")


```































***

## 01 Exploring and Preparing the Data

### Import Data

```{r}
# Import data
hr <- read.csv("IBM HR Data new.csv", na.strings = c("", "NA"))
str(hr)
summary(hr)
```

### Data Cleasing

```{r}
# Convert variables to factors or numerics
hr$Attrition <- as.factor(hr$Attrition)
hr$BusinessTravel <- as.factor(hr$BusinessTravel)
hr$DailyRate <- as.numeric(hr$DailyRate)
hr$Department <- as.factor(hr$Department)
hr$DistanceFromHome <- as.numeric(hr$DistanceFromHome)
hr$EducationField <- as.factor(hr$EducationField)
hr$Gender <- as.factor(hr$Gender)
hr$HourlyRate <- as.numeric(hr$HourlyRate)
hr$JobRole <- as.factor(hr$JobRole)
hr$JobSatisfaction <- as.numeric(hr$JobSatisfaction)
hr$MaritalStatus <- as.factor(hr$MaritalStatus)
hr$MonthlyIncome <- as.numeric(hr$MonthlyIncome)
hr$OverTime <- as.factor(hr$OverTime)
hr$PercentSalaryHike <- as.numeric(hr$PercentSalaryHike)
hr$Employee.Source <- as.factor(hr$Employee.Source)

# Remove non-relevant variables
hr$EmployeeCount <- NULL
hr$Application.ID <- NULL
hr$EmployeeNumber <- NULL
hr$Over18 <- NULL
hr$StandardHours <- NULL

# Remove all rows contain "Test"
hr <- hr[hr$EducationField != "Test", ]
hr <- hr[hr$Employee.Source != "Test", ]

# Remove all rows with NA values
hr <- hr[complete.cases(hr),]

summary(hr)
str(hr)
```

### Get Data Ready for Analysis

```{r}
# Convert Attrition variable to Yes/No levels
hr$Attrition <- as.factor(ifelse(hr$Attrition == "Voluntary Resignation", "Yes", "No"))

# Convert all factors into dummy variables as the input into ANN has to be numeric
hr_mm <- as.data.frame(model.matrix(~.-1,hr))

# Both AttritionYes and AttritionNo are created, so we remove AttritionNo column
hr_mm$AttritionNo <- NULL

# Remove the columns with NaN values
hr_mm$EducationFieldTest <- NULL
hr_mm$Gender2 <- NULL
hr_mm$JobRole5 <- NULL
hr_mm$OverTimeY <- NULL
hr_mm$Employee.Source2 <- NULL
hr_mm$Employee.SourceTest <- NULL

# Normalize the data for ANN and KNN
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

hr_norm <- as.data.frame(lapply(hr_mm, normalize))

# Summary of data for Logistic Regression and Decision Tree Model
str(hr_mm)
summary(hr_mm)

# Summary of data for ANN and KNN
str(hr_norm)
summary(hr_norm)
```

### Get Train and Test Samples

```{r}
# Selects 8000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(hr_mm), 8000) 

# Create a train set and test set for Decision Tree Model and Logistic Regression
hr_train <- hr_mm[-test_set, ]
hr_test <- hr_mm[test_set, ]

# Create a train set and test set for ANN and KNN
hr_norm_train <- hr_norm[-test_set, ]
hr_norm_test <- hr_norm[test_set, ]
```

***

## 02 Logistic Regression

### Train Model on Data

> We firstly run a logistic regression on all independent variables

```{r}
# Logistic model
lr_01 <- glm(AttritionYes ~ ., data = hr_train, family = "binomial")
summary(lr_01)
```

### Improve Model Fit using Stepwise Selection

> We use stepwise regression to find the optimal subset of independent variables

```{r results='hide'}
# Perform backward stepwise selection
lr_02 <- step(lr_01, direction = "backward")
summary(lr_02)
```

```{r}
# Logistic model with best subset using stepwise regression
lr_02 <- glm(AttritionYes ~ Age + BusinessTravelTravel_Frequently + 
               BusinessTravelTravel_Rarely + DailyRate + `DepartmentResearch & Development` + 
               DistanceFromHome + `EducationFieldHuman Resources` + `EducationFieldLife Sciences` + 
               EducationFieldMarketing + EducationFieldMedical + EducationFieldOther + 
               EnvironmentSatisfaction + GenderFemale + HourlyRate + JobInvolvement + 
               `JobRoleHealthcare Representative` + JobRoleManager + `JobRoleManufacturing Director` + 
               `JobRoleResearch Director` + `JobRoleResearch Scientist` + 
               `JobRoleSales Executive` + JobSatisfaction + MaritalStatusDivorced + 
               MaritalStatusMarried + NumCompaniesWorked + OverTimeYes + 
               PercentSalaryHike + PerformanceRating + RelationshipSatisfaction + 
               StockOptionLevel + TrainingTimesLastYear + WorkLifeBalance + 
               YearsInCurrentRole + YearsSinceLastPromotion + Employee.SourceAdzuna + 
               `Employee.SourceCompany Website` + Employee.SourceGlassDoor + 
               Employee.SourceIndeed + Employee.SourceJora + Employee.SourceReferral, 
             data = hr_train, family = "binomial")
summary(lr_02)
```

### Predict and Evaluate Model Performance

```{r}
# Predict probability on tele_test data based on model
predict.lr <- predict(lr_02, newdata = hr_test, type = "response")

# Classify predicted probability using CutOff = 0.25
lr_predict <- ifelse(predict.lr > 0.25, 1, 0)

# Confusion Matrix
library(caret)
Conf_Matrix_LR <- confusionMatrix(as.factor(lr_predict), hr_test$AttritionYes, positive = "1")
Conf_Matrix_LR

# Kappa Statistics
Conf_Matrix_LR$overall["Kappa"]
```

***

## 03 ANN

### Train Model on Data

```{r}
# Train the neuralnet model
library(neuralnet)

# Simple ANN with only a single hidden neuron
ann_01 <- neuralnet(formula = AttritionYes ~ ., data = hr_norm_train, stepmax=1e7)
```

### Predict and Evaluate Model Performance

```{r}
# Obtain model results
ann_model_results_01 <- compute(ann_01, hr_norm_test)

# Classify predicted probabilities using CutOff = 0.4
ann_predicted_01 <- ifelse(ann_model_results_01$net.result > 0.4, 1, 0)

# Confusion Matrix
Conf_Matrix_ANN_01 <- confusionMatrix(as.factor(ann_predicted_01), as.factor(hr_norm_test$AttritionYes), positive = "1")
Conf_Matrix_ANN_01

# Kappa Statistics
Conf_Matrix_ANN_01$overall["Kappa"]
```

***

## 04 KNN

### Train Model on Data

```{r}
# Exclude target variable from train and test data
hr_norm_train_knn <- hr_norm_train
hr_norm_train_knn$AttritionYes <- NULL

hr_norm_test_knn <- hr_norm_test
hr_norm_test_knn$AttritionYes <- NULL

# Store class labels for train and test data
hr_norm_train_labels <- hr_norm_train$AttritionYes
hr_norm_test_labels <- hr_norm_test$AttritionYes

# Find k value using the square root of the size of training data
sqrt(15217)

# Use KNN to classify test data
library(class)
knn_predict_01 <- knn(train = hr_norm_train_knn, test = hr_norm_test_knn,
                   cl = hr_norm_train_labels, k=123)
```

### Predict and Evaluate Model Performance

```{r}
# Confusion Matrix
Conf_Matrix_KNN_01 <- confusionMatrix(knn_predict_01, as.factor(hr_norm_test_labels), positive = "1")
Conf_Matrix_KNN_01

# Kappa Statistics
Conf_Matrix_KNN_01$overall["Kappa"]
```

### Prediction Improvement

```{r}
# Try different k values. False Negative is lowered when k = 21
knn_predict_02 <- knn(train = hr_norm_train_knn, test = hr_norm_test_knn,
                   cl = hr_norm_train_labels, k=21)

# Confusion Matrix
Conf_Matrix_KNN_02 <- confusionMatrix(knn_predict_02, as.factor(hr_norm_test_labels), positive = "1")
Conf_Matrix_KNN_02

# Kappa Statistics
Conf_Matrix_KNN_02$overall["Kappa"]
```

***

## 05 Decision Tree Model

### Train Model on Data

```{r}
library(C50)
hr_train$AttritionYes <- as.factor(hr_train$AttritionYes)

# Decision tree model
tree_model <- C5.0(AttritionYes ~ ., data = hr_train)
summary(tree_model)
```

### Predict and Evaluate Model Performance

```{r}
# Predict values
tree_predict <- predict(tree_model, hr_test)

# Confusion Matrix
Conf_Matrix_Tree <- confusionMatrix(tree_predict, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_Tree

# Kappa Statistics
Conf_Matrix_Tree$overall["Kappa"]
```

### Balance Between Different Errors

```{r}
# We want to lower False Negative (people predicted of staying at the company but actually leave)
# Cost of Error (Assigning a weight)
error_cost <- matrix(c(0, 1, 3, 0), nrow = 2)
error_cost

# New decision tree model by taking account cost of error
tree_cost_model <- C5.0(AttritionYes ~ ., data = hr_train, costs = error_cost)

# New predictions
tree_cost_predict <- predict(tree_cost_model, hr_test)

# Confusion Matrix of new model
Conf_Matrix_Tree_Cost <- confusionMatrix(tree_cost_predict, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_Tree_Cost

# Kappa Statistics of new model
Conf_Matrix_Tree_Cost$overall["Kappa"]
```

***

## 06 Support Vector Machine Model

### Train Model on Data

```{r}
library(kernlab)

# SVM Model (l)
SVM_model <- ksvm(AttritionYes ~ ., data = hr_train, kernel = "vanilladot")
SVM_model
```

### Predict and Evaluate Model Performance

```{r}
# Predict values
SVM_predictions <- predict(SVM_model, hr_test)

# Confusion Matrix
Conf_Matrix_SVM <- confusionMatrix(SVM_predictions, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_SVM

# Kappa Statistics
Conf_Matrix_SVM$overall["Kappa"]
```

### Prediction Improvement

```{r}
# Second SVM model (radial basis)
SVM_model_rbf <- ksvm(AttritionYes ~ ., data = hr_train, kernel = "rbfdot")

# Predict values
SVM_predictions_rbf <- predict(SVM_model_rbf, hr_test)

# Confusion Matrix
Conf_Matrix_SVM_rbf <- confusionMatrix(SVM_predictions_rbf, as.factor(hr_test$AttritionYes), positive = "1")
Conf_Matrix_SVM_rbf

# Kappa Statistics
Conf_Matrix_SVM_rbf$overall["Kappa"]
```
